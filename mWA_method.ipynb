{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64f2add3-e7b6-4ce4-b8e8-cc9d3520abc6",
   "metadata": {},
   "source": [
    "## L calculations using the formula $L = n \\cdot a_3$, where n are integers starting with 0 and $a_3$ is $a_3 = \\frac{\\lambda}{2(\\sin \\theta_2 - \\sin \\theta_1)}$, using the same values for theta limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f550d41-7fba-40c0-8ac8-706cc459d5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Assume calculate_common_L_grid is in the same file or imported\n",
    "\n",
    "def compute_common_L_grid(\n",
    "    df: pd.DataFrame,\n",
    "    lambda_nm: float = 0.01249, # wavelength value, should be changed for your case\n",
    "    two_theta1_deg: float = 3.0, # lower limit of your xrd pattern\n",
    "    two_theta2_deg: float = 7.5, # upper limit of your xrd pattern\n",
    "    n_max: int = 60\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Computes a common L-grid and broadcasts the values to all rows of a DataFrame.\n",
    "\n",
    "    This function acts as a wrapper around `calculate_common_L_grid`\n",
    "    to apply its results to a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df: The input DataFrame.\n",
    "        lambda_nm: Wavelength in nanometers.\n",
    "        two_theta1_deg: The first 2θ gate in degrees.\n",
    "        two_theta2_deg: The second 2θ gate in degrees.\n",
    "        n_max: The maximum integer 'n' for the grid.\n",
    "\n",
    "    Adds to DataFrame\n",
    "    -----------------\n",
    "    - 'a3_nm' (float): The calculated 'a3' spacing, same for all rows.\n",
    "    - 'L_common_nm' (list[float]): The L-grid list [n*a3], same for all rows.\n",
    "\n",
    "    Returns:\n",
    "        A copy of the input DataFrame with 'a3_nm' and 'L_common_nm' columns added.\n",
    "    \"\"\"\n",
    "    # 1. Perform the core calculation\n",
    "    a3_nm, L_list = calculate_common_L_grid(\n",
    "        lambda_nm=lambda_nm,\n",
    "        two_theta1_deg=two_theta1_deg,\n",
    "        two_theta2_deg=two_theta2_deg,\n",
    "        n_max=n_max\n",
    "    )\n",
    "\n",
    "    # 2. Apply to the DataFrame\n",
    "    out = df.copy()\n",
    "    out['a3_nm'] = a3_nm\n",
    "\n",
    "    # Create a *new* list for each row to prevent mutation issues\n",
    "    # (This matches your original, safe implementation)\n",
    "    out['L_common_nm'] = [list(L_list) if L_list is not None else None for _ in range(len(out))]\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777ad844-2681-4e74-abc1-83a86d5779dc",
   "metadata": {},
   "source": [
    "## Calculating A(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17d32d0-bbda-4527-b05c-9073c3084854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------- Helpers ----------------------\n",
    "\n",
    "def _uniform_q_and_intensity(theta_rad, inten, theta1_rad, theta2_rad, thetaB_rad, lambda_nm):\n",
    "    \"\"\"\n",
    "    Slice by [theta1, theta2], convert to k and recenter to q=k-kB.\n",
    "    Return uniform q in a symmetric window [-Q, Q] and baseline-corrected, area-normalized intensity.\n",
    "    \"\"\"\n",
    "    # 1) Slice the peak window in theta (radians)\n",
    "    mask = (theta_rad >= theta1_rad) & (theta_rad <= theta2_rad)\n",
    "    if not np.any(mask):\n",
    "        return None, None\n",
    "\n",
    "    th = theta_rad[mask]\n",
    "    I = inten[mask]\n",
    "\n",
    "    # 2) Convert to k (1/nm) and recenter to q = k - kB\n",
    "    k  = 2.0 * np.sin(th) / lambda_nm\n",
    "    kB = 2.0 * np.sin(thetaB_rad) / lambda_nm\n",
    "    q = k - kB\n",
    "\n",
    "    # 3) Symmetric window around q=0: [-Q, Q]\n",
    "    Q = min(np.max(q), -np.min(q))\n",
    "    if not np.isfinite(Q) or Q <= 0:\n",
    "        return None, None\n",
    "    keep = (q >= -Q) & (q <= Q)\n",
    "    q = q[keep]\n",
    "    I = I[keep]\n",
    "\n",
    "    # 4) Interpolate to a *uniform* q-grid, enforce an odd length so 0 is included\n",
    "    if q.size < 3:\n",
    "        return None, None\n",
    "    N = max(3, int(q.size) | 1)  # make odd\n",
    "    q_uniform = np.linspace(-Q, Q, N)\n",
    "    I_uniform = np.interp(q_uniform, q, I)\n",
    "\n",
    "    # 5) Baseline: straight line through the two ends -> subtract -> clip ≥ 0\n",
    "    I0, I1   = I_uniform[0], I_uniform[-1]\n",
    "    baseline = I0 + (I1 - I0) * (q_uniform - q_uniform[0]) / (q_uniform[-1] - q_uniform[0])\n",
    "    I_corr   = I_uniform - baseline\n",
    "    I_corr[I_corr < 0] = 0.0\n",
    "\n",
    "    # 6) Area-normalize so ∑ I dq = 1  → ensures A(0)=1\n",
    "    dq   = q_uniform[1] - q_uniform[0]\n",
    "    area = np.sum(I_corr) * dq\n",
    "    if area <= 0 or not np.isfinite(area):\n",
    "        return None, None\n",
    "    I_norm = I_corr / area\n",
    "\n",
    "    return q_uniform, I_norm\n",
    "\n",
    "\n",
    "def _A_on_L(q, I_norm, L_nm):\n",
    "    \"\"\"\n",
    "    Compute A(L) = ∫ I(q) cos(2π L q) dq ≈ sum I(q) cos(2π L q) dq for L in nm, q in 1/nm.\n",
    "    Returns (A_list, lnA_list) where lnA is NaN wherever A <= 0.\n",
    "    \"\"\"\n",
    "    if q is None or I_norm is None or len(L_nm) == 0:\n",
    "        return None, None\n",
    "\n",
    "    q      = np.asarray(q, float)\n",
    "    I_norm = np.asarray(I_norm, float)\n",
    "    L_arr  = np.asarray(L_nm, float)\n",
    "    dq     = q[1] - q[0]\n",
    "\n",
    "    # Broadcast over L: cos(2π L q)\n",
    "    cos_matrix = np.cos(2.0 * np.pi * np.outer(L_arr, q))\n",
    "    A = cos_matrix @ (I_norm * dq)  # (len(L),)\n",
    "\n",
    "    # Numerical safety: allow zeros, set lnA to NaN where A <= 0\n",
    "    A = np.where(A < 0, 0.0, A)\n",
    "    lnA = np.where(A > 0, np.log(A), np.nan)\n",
    "\n",
    "    return A.tolist(), lnA.tolist()\n",
    "\n",
    "\n",
    "# ---------------------- Main ----------------------\n",
    "\n",
    "def compute_A_for_all_peaks_commonL(\n",
    "    df: pd.DataFrame,\n",
    "    dat_dir: str,\n",
    "    lambda_nm: float = 0.01249,\n",
    "    # If L_common_nm is missing, we will build it from these 2θ gates:\n",
    "    two_theta1_deg: float = 3.0,\n",
    "    two_theta2_deg: float = 7.5,\n",
    "    n_max: int = 60,\n",
    "    # Optional raw scan truncation (2θ ≤ tth_max_deg). Set to None to disable.\n",
    "    tth_max_deg: float | None = 7.5,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute A(L) and ln A(L) for peaks 1..4 on a single *shared* L-grid.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    df : DataFrame with one row per scan. Required columns:\n",
    "      - x{i}                (2θ center in degrees) for i=1..4\n",
    "      - Option A (preferred): per-peak windows in radians\n",
    "          theta1_p{i}_rad, theta2_p{i}_rad\n",
    "        OR\n",
    "      - Option B (fallback): none of the above → we use common gates:\n",
    "          2θ ∈ [two_theta1_deg, two_theta2_deg] for every peak.\n",
    "\n",
    "      - Optional: L_common_nm (list of floats). If present, it is used directly.\n",
    "        If absent, it is created from:\n",
    "          a3 = λ / [2 (sin(θ2) - sin(θ1))],  with θ = (2θ)/2\n",
    "          L_n = n * a3 for n=0..n_max\n",
    "\n",
    "    dat_dir : folder with one .dat per row, sorted lexicographically and aligned\n",
    "              to df order. Each .dat has two columns: \"2θ(deg)  I\".\n",
    "\n",
    "    Outputs (added columns)\n",
    "    -----------------------\n",
    "      - 'a3_nm' (float, only if L_common_nm was created here)\n",
    "      - 'L_common_nm' (list[float])  # the shared L-grid (same list in all rows)\n",
    "      - 'A_p{i}_list', 'lnA_p{i}_list' for i=1..4 (lists evaluated on L_common_nm)\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    # --- 0) Ensure or create the shared L-grid --------------------------------\n",
    "    if 'L_common_nm' in out.columns and isinstance(out['L_common_nm'].iat[0], (list, np.ndarray)):\n",
    "        L_common = np.asarray(out['L_common_nm'].iat[0], float)\n",
    "    else:\n",
    "        theta1_rad = np.deg2rad(two_theta1_deg / 2.0)\n",
    "        theta2_rad = np.deg2rad(two_theta2_deg / 2.0)\n",
    "        delta = np.sin(theta2_rad) - np.sin(theta1_rad)\n",
    "        if delta == 0:\n",
    "            raise ValueError(\"sin(theta2) - sin(theta1) == 0; cannot build common L-grid.\")\n",
    "        a3_nm = lambda_nm / (2.0 * abs(delta))\n",
    "        n = np.arange(0, n_max + 1, dtype=int)\n",
    "        L_common = n * a3_nm\n",
    "        out['a3_nm'] = a3_nm\n",
    "        out['L_common_nm'] = [L_common.tolist() for _ in range(len(out))]\n",
    "\n",
    "    # --- 1) Collect and align .dat files --------------------------------------\n",
    "    files = sorted(glob.glob(os.path.join(dat_dir, \"*.dat\")))\n",
    "    if len(files) < len(out):\n",
    "        raise ValueError(f\"Found {len(files)} .dat files, but dataframe has {len(out)} rows.\")\n",
    "    files = files[:len(out)]\n",
    "\n",
    "    # Prepare output columns\n",
    "    for p in range(1, 5):\n",
    "        out[f\"A_p{p}_list\"] = None\n",
    "        out[f\"lnA_p{p}_list\"] = None\n",
    "\n",
    "    # --- 2) Row-wise processing ------------------------------------------------\n",
    "    for idx, path in enumerate(files):\n",
    "        # Load dat file: first two columns -> tth_deg, I_raw\n",
    "        dat = pd.read_csv(\n",
    "            path, sep=r\"\\s+\", header=None, comment=\"#\",\n",
    "            names=[\"tth_deg\", \"I_raw\"], engine=\"python\"\n",
    "        )\n",
    "        if tth_max_deg is not None:\n",
    "            dat = dat[dat[\"tth_deg\"] <= float(tth_max_deg)]\n",
    "        if dat.empty:\n",
    "            continue\n",
    "\n",
    "        theta_rad_all = np.deg2rad(dat[\"tth_deg\"].to_numpy(dtype=float) / 2.0)\n",
    "        I_all = dat[\"I_raw\"].to_numpy(dtype=float)\n",
    "\n",
    "        for p in range(1, 5):\n",
    "            # Peak center (2θ deg) -> θB (rad)\n",
    "            if f\"x{p}\" not in out.columns:\n",
    "                continue\n",
    "            x2theta_deg = out.at[idx, f\"x{p}\"]\n",
    "            if not np.isfinite(x2theta_deg):\n",
    "                continue\n",
    "            thetaB_rad = np.deg2rad(x2theta_deg / 2.0)\n",
    "\n",
    "            # Peak window: prefer per-peak theta1/theta2 if present, else fallback to common gates\n",
    "            if f\"theta1_p{p}_rad\" in out.columns and f\"theta2_p{p}_rad\" in out.columns:\n",
    "                th1 = out.at[idx, f\"theta1_p{p}_rad\"]\n",
    "                th2 = out.at[idx, f\"theta2_p{p}_rad\"]\n",
    "                if not (np.isfinite(th1) and np.isfinite(th2)):\n",
    "                    th1 = np.deg2rad(two_theta1_deg / 2.0)\n",
    "                    th2 = np.deg2rad(two_theta2_deg / 2.0)\n",
    "            else:\n",
    "                th1 = np.deg2rad(two_theta1_deg / 2.0)\n",
    "                th2 = np.deg2rad(two_theta2_deg / 2.0)\n",
    "\n",
    "            q_uniform, I_norm = _uniform_q_and_intensity(\n",
    "                theta_rad_all, I_all, th1, th2, thetaB_rad, lambda_nm\n",
    "            )\n",
    "            if q_uniform is None:\n",
    "                continue\n",
    "\n",
    "            A_list, lnA_list = _A_on_L(q_uniform, I_norm, L_common)\n",
    "            out.at[idx, f\"A_p{p}_list\"] = A_list\n",
    "            out.at[idx, f\"lnA_p{p}_list\"] = lnA_list\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6494491f-ec03-46b2-9c04-6163969bdf26",
   "metadata": {},
   "source": [
    "## logA vs K^2C -> slope is -ML^2ln(R_e/L), intercept is logA_s(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a566e28-9d7b-4e62-bef3-10d64d54b642",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------\n",
    "# User inputs / constants\n",
    "# ------------------------\n",
    "# wavelength in nm (example: 99.3 keV -> ~0.012486 nm). Change to yours.\n",
    "WAVELENGTH_NM = 0.012486  # [nm]\n",
    "\n",
    "# Mean contrast factors per peak (order must match x1..x4 and A_p1_list..A_p4_list)\n",
    "C_MEAN = np.array([0.141, 0.28485, 0.14105, 0.141], dtype=float)\n",
    "\"\"\"\n",
    "The contrast value C_MEAN is set to a fixed value, just to test the script. It should be calculated and fitted for temperature as shown in\n",
    "modified Williamson-Hall method. Contrast factor values can be calculated from ANIZC website.\n",
    "\"\"\"\n",
    "\n",
    "# Column names (change if yours differ)\n",
    "X_COLS = ('x1', 'x2', 'x3', 'x4')  # peak centers in 2θ degrees\n",
    "A_COLS = ('A_p1_list', 'A_p2_list', 'A_p3_list', 'A_p4_list')  # lists of A(L) per peak\n",
    "LCOL   = 'L_common_nm'  # shared L-grid column (list of floats, same for all rows)\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Helpers\n",
    "# ------------------------\n",
    "def two_theta_deg_to_K(two_theta_deg, lambda_nm):\n",
    "    \"\"\"\n",
    "    Convert 2θ (degrees) -> K [1/nm]\n",
    "    K = 2 * sin(theta) / lambda, with theta = (2θ)/2 in radians.\n",
    "    \"\"\"\n",
    "    theta_rad = 0.5 * np.deg2rad(two_theta_deg)\n",
    "    return 2.0 * np.sin(theta_rad) / lambda_nm\n",
    "\n",
    "\n",
    "def wa_inner_fits_for_row(\n",
    "    row,\n",
    "    wavelength_nm,\n",
    "    c_mean,\n",
    "    x_cols,\n",
    "    a_cols,\n",
    "    lcol=LCOL,\n",
    "    # --- plotting controls ---\n",
    "    save_plots=True,\n",
    "    output_dir=\"wa_inner_plots\",\n",
    "    save_every_n=500,\n",
    "    dpi=140\n",
    "):\n",
    "    \"\"\"\n",
    "    For a single row:\n",
    "      - Read the shared L-grid from row[lcol].\n",
    "      - Compute K_j and X_j = K_j^2 * C_j  (fixed across L)\n",
    "      - For each L_k, fit log A_j(L_k) vs X_j across peaks j=1..4\n",
    "          * quadratic polyfit (deg=2) -> use constant & linear terms (a0, a1)\n",
    "          * strict linear fit (deg=1)\n",
    "      - Save every `save_every_n`-th plot with both fits overlaid (if save_plots=True).\n",
    "    Returns a dict of arrays aligned with the per-row L grid.\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists if plotting\n",
    "    if save_plots:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # --- 0) Get the shared L-grid from the row ---\n",
    "    if lcol not in row or row[lcol] is None:\n",
    "        raise ValueError(f\"Missing shared L grid column '{lcol}' in row {row.name}.\")\n",
    "    L_grid = np.asarray(row[lcol], dtype=float)\n",
    "    if L_grid.ndim != 1 or L_grid.size == 0:\n",
    "        raise ValueError(f\"Row {row.name}: '{lcol}' must be a non-empty 1D list/array.\")\n",
    "\n",
    "    # --- 1) Build X_j = K^2 * C_mean (j=1..4), fixed over L ---\n",
    "    two_theta = np.array([row[col] for col in x_cols], dtype=float)  # [deg]\n",
    "    if not np.all(np.isfinite(two_theta)):\n",
    "        raise ValueError(f\"Row {row.name}: non-finite 2θ center in {x_cols}.\")\n",
    "    if len(c_mean) != len(x_cols):\n",
    "        raise ValueError(\"C_MEAN length must match number of peaks.\")\n",
    "    K = two_theta_deg_to_K(two_theta, wavelength_nm)                 # [1/nm]\n",
    "    X = (K**2) * c_mean                                              # [1/nm^2] * (dimensionless)\n",
    "\n",
    "    # --- 2) Get A_j(L) lists, validate lengths vs L_grid ---\n",
    "    A_lists = []\n",
    "    for idx_p, col in enumerate(a_cols, start=1):\n",
    "        if col not in row or row[col] is None:\n",
    "            raise ValueError(f\"Row {row.name}: missing A(L) list in column '{col}'.\")\n",
    "        Aj = np.asarray(row[col], dtype=float)\n",
    "        if Aj.ndim != 1 or Aj.size != L_grid.size:\n",
    "            raise ValueError(\n",
    "                f\"Row {row.name}: A_p{idx_p}_list length ({Aj.size}) != L_common length ({L_grid.size}).\"\n",
    "            )\n",
    "        A_lists.append(Aj)\n",
    "\n",
    "    nL = L_grid.size\n",
    "\n",
    "    # --- 3) Prepare outputs ---\n",
    "    m_poly2 = np.full(nL, np.nan, dtype=float)  # slope from quadratic fit (linear term)\n",
    "    c_poly2 = np.full(nL, np.nan, dtype=float)  # intercept from quadratic fit (constant term)\n",
    "    m_lin   = np.full(nL, np.nan, dtype=float)  # slope from strict linear fit\n",
    "    c_lin   = np.full(nL, np.nan, dtype=float)  # intercept from strict linear fit\n",
    "\n",
    "    # File-safe row ID\n",
    "    row_id = str(row.name).replace(os.sep, \"_\").replace(\" \", \"_\")\n",
    "\n",
    "    # --- 4) Loop over L_k and fit across peaks (j) ---\n",
    "    for k in range(nL):\n",
    "        # y_j = log A_j(L_k); require all finite and > 0\n",
    "        y_vals = np.array([Aj[k] for Aj in A_lists], dtype=float)\n",
    "        if (y_vals <= 0).any() or (~np.isfinite(y_vals)).any():\n",
    "            # invalid for log; leave NaNs\n",
    "            continue\n",
    "        y = np.log(y_vals)\n",
    "        x = X\n",
    "\n",
    "        # Quadratic polyfit: y ~ a2*x^2 + a1*x + a0\n",
    "        coeff2 = np.polyfit(x, y, deg=2)\n",
    "        a2, a1, a0 = coeff2\n",
    "        m_poly2[k] = a1\n",
    "        c_poly2[k] = a0\n",
    "\n",
    "        # Strict linear fit: y ~ m*x + c\n",
    "        m1, c1 = np.polyfit(x, y, deg=1)\n",
    "        m_lin[k] = m1\n",
    "        c_lin[k] = c1\n",
    "\n",
    "        # ---- Save every Nth plot (overlay data + both fits) ----\n",
    "        if save_plots and (k % save_every_n == 0):\n",
    "            x_fit = np.linspace(x.min(), x.max(), 200)\n",
    "            y_fit_lin = m1 * x_fit + c1\n",
    "            y_fit_quad = (a2 * x_fit**2) + (a1 * x_fit) + a0\n",
    "\n",
    "            plt.figure(figsize=(6.5, 4.5))\n",
    "            plt.scatter(x, y, label=r'$\\log A_j(L_k)$ data', zorder=3)\n",
    "            plt.plot(x_fit, y_fit_lin, label='Linear fit', linewidth=1.6)\n",
    "            plt.plot(x_fit, y_fit_quad, label='Quadratic fit', linewidth=1.6)\n",
    "            plt.xlabel(r'$X = K^2 C$  [1/nm$^2$]')\n",
    "            plt.ylabel(r'$\\log A_j(L)$')\n",
    "            plt.title(f'Row {row_id} | L[{k}] = {L_grid[k]:.4f} nm')\n",
    "            plt.grid(True, linestyle='--', alpha=0.35)\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "\n",
    "            fname = os.path.join(\n",
    "                output_dir,\n",
    "                f\"row{row_id}_Lidx{k:05d}_Lnm{L_grid[k]:.4f}.png\"\n",
    "            )\n",
    "            plt.savefig(fname, dpi=dpi)\n",
    "            plt.close()\n",
    "\n",
    "    return {\n",
    "        'L_grid': np.array(L_grid, dtype=float),  # [nm]\n",
    "        'm_poly2': m_poly2,  # slope at each L_k (quadratic fit's linear term)\n",
    "        'c_poly2': c_poly2,  # intercept at each L_k (quadratic fit's constant)\n",
    "        'm_lin': m_lin,      # slope at each L_k (strict linear fit)\n",
    "        'c_lin': c_lin,      # intercept at each L_k (strict linear fit)\n",
    "        'K_values': K,       # per-peak K [1/nm]\n",
    "        'X_K2C': X,          # per-peak X = K^2 * C_mean\n",
    "    }\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Apply to a DataFrame `df`\n",
    "# (Assumes `df` has columns: x1..x4, A_p1_list..A_p4_list, and L_common_nm)\n",
    "# ------------------------\n",
    "def compute_wa_inner_fits(\n",
    "    df: pd.DataFrame,\n",
    "    save_plots=True,\n",
    "    output_dir=\"wa_inner_plots\",\n",
    "    save_every_n=500,\n",
    "    dpi=140\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds these columns per row:\n",
    "      - 'L_grid' : np.ndarray of L values [nm] (copied from 'L_common_nm')\n",
    "      - 'm_poly2', 'c_poly2', 'm_lin', 'c_lin' : np.ndarray per row, aligned with L_grid\n",
    "      - 'K_values', 'X_K2C' : diagnostic arrays per row\n",
    "    Also saves every `save_every_n`th plot with both fits, if save_plots=True.\n",
    "    \"\"\"\n",
    "    # Quick pre-checks\n",
    "    missing = [col for col in (*X_COLS, *A_COLS, LCOL) if col not in df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "    results = df.apply(\n",
    "        lambda row: pd.Series(\n",
    "            wa_inner_fits_for_row(\n",
    "                row,\n",
    "                wavelength_nm=WAVELENGTH_NM,\n",
    "                c_mean=C_MEAN,\n",
    "                x_cols=X_COLS,\n",
    "                a_cols=A_COLS,\n",
    "                lcol=LCOL,\n",
    "                save_plots=save_plots,\n",
    "                output_dir=output_dir,\n",
    "                save_every_n=save_every_n,\n",
    "                dpi=dpi\n",
    "            )\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Concatenate results back\n",
    "    for col in ['L_grid', 'm_poly2', 'c_poly2', 'm_lin', 'c_lin', 'K_values', 'X_K2C']:\n",
    "        df[col] = results[col]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a733ca3b-b1e7-4578-93a3-2fbfc7a4767c",
   "metadata": {},
   "source": [
    "## Cut-off radius ($R_e$) and dislcoation density ($\\rho$) calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4280e2f-d515-4921-bdfb-fc9cea19e6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# -----------------------------\n",
    "# WA outer fit on Y(L) vs L\n",
    "#   Y(L) = -m(L)/L^2\n",
    "#   Model: Y(L) = M * ln(Re / L)\n",
    "#   Then:  rho = 2M / (pi*b^2)\n",
    "# Keep L in nm, b in nm -> rho in nm^-2 (×1e18 for m^-2)\n",
    "# -----------------------------\n",
    "\n",
    "B_NM_DEFAULT = 0.248  # Burgers vector (nm)\n",
    "\n",
    "def _model_Y(L, M, Re):\n",
    "    return M * np.log(Re / L)\n",
    "\n",
    "def _initial_guess_Y(L, Y):\n",
    "    Lmax = float(np.max(L))\n",
    "    Re0 = 5.0 * Lmax  # start with Re well above data range\n",
    "    denom = np.log(Re0 / L)\n",
    "    denom[~np.isfinite(denom) | (denom <= 0)] = np.nan\n",
    "    M0 = np.nanmedian(Y / denom)\n",
    "    if not np.isfinite(M0) or M0 <= 0:\n",
    "        M0 = 1e-4\n",
    "    return M0, Re0\n",
    "\n",
    "def _fit_Y_vs_L(L, m_series, b_nm=B_NM_DEFAULT,\n",
    "                require_positive_Y=True, L_min=None, L_max=None):\n",
    "    \"\"\"\n",
    "    Fits Y(L) = -m(L)/L^2 to Y = M * ln(Re/L).\n",
    "    Returns (rho_nm^-2, Re_nm, R2).\n",
    "    \"\"\"\n",
    "    L = np.asarray(L, dtype=float)\n",
    "    m = np.asarray(m_series, dtype=float)\n",
    "\n",
    "    mask = np.isfinite(L) & np.isfinite(m) & (L > 0)\n",
    "    if L_min is not None:\n",
    "        mask &= (L >= float(L_min))\n",
    "    if L_max is not None:\n",
    "        mask &= (L <= float(L_max))\n",
    "    if mask.sum() < 4:\n",
    "        return np.nan, np.nan, np.nan\n",
    "\n",
    "    L_use = L[mask]\n",
    "    Y = -m[mask] / (L_use**2)\n",
    "\n",
    "    if require_positive_Y:\n",
    "        keep = np.isfinite(Y) & (Y > 0)\n",
    "    else:\n",
    "        keep = np.isfinite(Y)\n",
    "    L_use, Y = L_use[keep], Y[keep]\n",
    "    if L_use.size < 4:\n",
    "        return np.nan, np.nan, np.nan\n",
    "\n",
    "    # Initial guesses & bounds\n",
    "    M0, Re0 = _initial_guess_Y(L_use, Y)\n",
    "    lower = [0.0, 1.01 * float(np.max(L_use))]     # M>=0, Re > max L\n",
    "    upper = [np.inf, 1e6 * float(np.max(L_use))]\n",
    "\n",
    "    try:\n",
    "        popt, _ = curve_fit(\n",
    "            _model_Y, L_use, Y, p0=[M0, Re0],\n",
    "            bounds=(lower, upper), maxfev=20000\n",
    "        )\n",
    "        M, Re = popt\n",
    "        Yfit = _model_Y(L_use, M, Re)\n",
    "        ss_res = np.sum((Y - Yfit)**2)\n",
    "        ss_tot = np.sum((Y - np.mean(Y))**2)\n",
    "        R2 = 1.0 - ss_res/ss_tot if ss_tot > 0 else np.nan\n",
    "\n",
    "        rho_nm2 = (2.0 * M) / (np.pi * (b_nm**2))\n",
    "        return rho_nm2, float(Re), R2\n",
    "    except Exception:\n",
    "        return np.nan, np.nan, np.nan\n",
    "\n",
    "def wa_outer_fit_Y_vs_L_YvsLcols(df, b_nm=B_NM_DEFAULT,\n",
    "                                 require_positive_Y=True, L_min=None, L_max=None):\n",
    "    \"\"\"\n",
    "    Applies the Y vs L (nonlinear) WA outer fit to both m_lin and m_poly2.\n",
    "    Saves to NEW columns that make it explicit this fit uses L (not ln L):\n",
    "      - 'rho_mlin_nm^-2_YvsL',   'Re_mlin_nm_YvsL',   'r2_mlin_YvsL'\n",
    "      - 'rho_mpoly2_nm^-2_YvsL', 'Re_mpoly2_nm_YvsL', 'r2_mpoly2_YvsL'\n",
    "    \"\"\"\n",
    "    def _row_fit(row):\n",
    "        out = {}\n",
    "\n",
    "        rho1, Re1, R21 = _fit_Y_vs_L(\n",
    "            row['L_grid'], row['m_lin'], b_nm=b_nm,\n",
    "            require_positive_Y=require_positive_Y, L_min=L_min, L_max=L_max\n",
    "        )\n",
    "        out['rho_mlin_nm^-2_YvsL'] = rho1\n",
    "        out['Re_mlin_nm_YvsL']     = Re1\n",
    "        out['r2_mlin_YvsL']        = R21\n",
    "\n",
    "        rho2, Re2, R22 = _fit_Y_vs_L(\n",
    "            row['L_grid'], row['m_poly2'], b_nm=b_nm,\n",
    "            require_positive_Y=require_positive_Y, L_min=L_min, L_max=L_max\n",
    "        )\n",
    "        out['rho_mpoly2_nm^-2_YvsL'] = rho2\n",
    "        out['Re_mpoly2_nm_YvsL']     = Re2\n",
    "        out['r2_mpoly2_YvsL']        = R22\n",
    "\n",
    "        return pd.Series(out)\n",
    "\n",
    "    results = df.apply(_row_fit, axis=1)\n",
    "    return pd.concat([df, results], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fc7ff5-5b41-45eb-8f31-2d25f49498c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306c6107-5348-4189-9a3e-47d45a1c15bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3595b09-bb69-467e-b10a-2a19adcec808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9fac47-bd0d-45cd-bbe8-3fcdc51a5d3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4981c615-7454-4c6f-a21c-2c2162a9a07b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feaff62-5b27-4981-88db-c7bf281a4851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8610f7-7bf6-4e05-967d-c0def24eb1b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9bb90d-4cc7-409f-9ba5-ecfddfe5b574",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec5f43a-9dfa-4af7-8f3b-0e7564fb39c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f8ccc5-28b4-4e7b-b1bc-770752a73fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3e4453-db35-40a8-bcf1-a28cdf9b0002",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6811b816-6dfe-4274-b658-77ee683dfc4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb98645-c8d2-46a6-bfd3-15a0a3b7bf8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d030cc-d05f-49ef-a3d9-e5ec103db1ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1ba4ad-f748-4956-9ef3-7a87b0b71100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f22eddf-5c7f-411c-aaab-57d8eeac85c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd531556-36a2-4553-bb77-7f24c2cc88c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40856ed1-f36d-4f70-be1b-295779a52302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391dda10-9b6c-4489-8b7a-4474fdefa7a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b435e7d-1ebb-4828-9eca-89713e24fcc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9064d471-a650-448d-901b-1a6d1339fa7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3632ba-0eda-4da8-91e8-0e8bf8a3e1ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
